import warnings

import numpy as np

from untitlednn.initializer import RandomInitializer, ZeroInitializer
from untitlednn.autodiff import AutoDiff, tensor, Tensor, identity, Executor


class Layer(object):
    """Base Layer"""

    def __init__(self, name):
        self.name = name
        self.params = {}
        self.grads = {}
        self.inputs = None

        self.in_shape = None
        self.out_shape = None

        # for auto diff
        self.auto_diff_obj = None
        self.forward_output = None

        self.__param_num = None

    # @profile    # https://github.com/pythonprofilers/memory_profiler
    def forward_with_autodiff(self, inputs):
        """
        forward_with_autodiff æ˜¯å¯¹ forward çš„è‡ªåŠ¨å¾®åˆ†å°è£…
        """
        self.inputs = tensor(inputs)

        with AutoDiff(self.inputs) as ad:
            f = self.forward(self.inputs)

        self.auto_diff_obj = ad
        self.forward_output = f

        # A BED IMPLEMENT: return f
        # è¿™é‡Œè¦è¿”å›žä¸ªæ–°çš„ tensor, æ–­å¼€ä¸Žä¸‹ä¸€å±‚çš„è”ç³»ã€‚
        # ä¸ç„¶åŽé¢çš„å±‚æŠŠ f ä½œä¸ºè¾“å…¥, f (å³è¿™ä¸€å±‚çš„ forward_output) ä¼šè¢«ä¸‹ä¸€å±‚çš„ AutoDiff
        # ç½®ä¸º identityï¼Œç„¶åŽå½“å‰å±‚éœ€è¦ä¿ç•™çš„è®¡ç®—å›¾è¿žæŽ¥ (backward è®¡ç®—æ¢¯åº¦éœ€è¦çš„
        # inputs -> ... -> forward_output ) å°±ä¸¢å¤±äº†ã€‚
        # åŒæ—¶, å±‚ä¸Žå±‚ç›´æŽ¥ä¸ç›´æŽ¥è¿žæŽ¥æœ‰åŠ©äºŽå†…å­˜ä¼˜åŒ–, see Layer.backward_with_clean
        return tensor(f)

    def forward(self, inputs):
        """forward æ˜¯å‘å‰ä¼ æ’­çš„å…·ä½“ç®—æ³•

        é€šè¿‡é‡è½½æ¥å®žçŽ°

        :param inputs: è¾“å…¥å€¼
        :return: å±‚çš„å‰å‘è®¡ç®—è¾“å‡ºå€¼
        """
        raise NotImplementedError

    # @profile    # https://github.com/pythonprofilers/memory_profiler
    def backward(self, grads):
        """backward åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚

        é»˜è®¤é€šè¿‡ forward è‡ªåŠ¨å¾®åˆ†è®¡ç®—ï¼Œé‡è½½æ¥è‡ªå®šä¹‰ã€‚

        :param grads: è¾“å‡ºï¼ˆä¸‹ä¸€å±‚ï¼‰çš„æ¢¯åº¦
        :return: è¿™ä¸€å±‚çš„æ¢¯åº¦
        """
        g = self.auto_diff_obj.gradient(self.forward_output, self.inputs, output_grad=grads)
        g = tensor(g)

        for key in self.params:
            # self.grads[key] = self.auto_diff_obj.gradient(self.forward_output, self.params[key], output_grad=grads)
            # ðŸ‘‡ä¸‹é¢è¿™è¡Œä»£ç ç­‰äºŽä¸Šé¢çš„è¿™è¡ŒðŸ‘†ï¼Œå‡å°‘å‡½æ•°è°ƒç”¨
            self.grads[key] = tensor(self.params[key].grad)

        # æ–­å¼€è®¡ç®—å›¾çš„è¿žæŽ¥
        self.auto_diff_obj.close()

        return g

    # @profile    # https://github.com/pythonprofilers/memory_profiler
    def backward_with_clean(self, grads):
        """Deprecated

        è°ƒç”¨ self.backward ç„¶åŽåšå†…å­˜æ¸…ç†å·¥ä½œã€‚

        è¿™ä¸ªåŠŸèƒ½åœ¨ Layer.backward ä¸­å®žçŽ°äº†, å› æ­¤ä¸å†ä½¿ç”¨æ­¤æ–¹æ³•
        """
        warnings.warn("backward_with_clean is deprecated. "
                      "è¿™ä¸ªåŠŸèƒ½åœ¨ Layer.backward ä¸­å®žçŽ°äº†, å› æ­¤ä¸å†ä½¿ç”¨æ­¤æ–¹æ³•",
                      DeprecationWarning)

        g = tensor(self.backward(tensor(grads)))

        # # 0. clean up: of no avail
        # stack = [self.forward_output]
        # while stack:
        #     i = stack.pop(0)
        #     # print(len(stack), id(i), type(i), i.id if isinstance(i, Tensor) else -1, sys.getrefcount(i))
        #     if not isinstance(i, Tensor):
        #         del i
        #         continue
        #     elif i.op == identity:
        #         continue
        #     stack.extend(i.inputs)
        #     del i
        # # print('----')

        # # 1. del & call gc: useless
        # del self.inputs.grad
        # del self.forward_output
        # del self.auto_diff_obj
        #
        # n = gc.collect()
        # if n:
        #     print('gc:', n)

        # # 2. THIS WORKS!
        # # æ–­å¼€è®¡ç®—å›¾çš„è¿žæŽ¥: è¿™ä¸ªæ˜¯å†…å­˜ä¼˜åŒ–çš„å…³é”®ï¼ï¼ï¼
        # # è®¡ç®—å›¾èŠ‚ç‚¹ (å³ Tensor) ç›¸äº’å¼•ç”¨, é€ æˆè¿™äº›ä¸å†ä½¿ç”¨çš„èŠ‚ç‚¹, æ— æ³•åŠæ—¶è¢« GC æ¸…ç†,
        # # è¿™äº›æ— æ³•å†æ¬¡ä½¿ç”¨çš„è®¡ç®—å›¾èŠ‚ç‚¹, ä¼šé•¿æ—¶é—´é©»ç•™å†…å­˜, ç›´åˆ° model.fit çš„ for epoch
        # # è®­ç»ƒå¾ªçŽ¯å®Œå…¨ç»“æŸã€‚
        # # è¿™ä¸ªå†…å­˜æ³„éœ²é—®é¢˜ä¼šå¯¼è‡´å†…å­˜å ç”¨è¶…å‡ºé¢„æœŸæ•°åå€, ç”¨ schoolwork è®­ç»ƒ 10 è½®ä½œä¸ºæ¯”
        # # è¾ƒ, ä¸‹é¢çš„ä»£ç å¯ä»¥ä½¿å†…å­˜å³°å€¼ä»Ž 3GiB é™ä½Žåˆ° 120 MiBã€‚
        # ex = Executor(self.forward_output)
        # # print(len(ex.topo_list))
        # for i in ex.topo_list:
        #     if isinstance(i, Tensor):
        #         i.inputs = []

        # æŠŠæ–¹æ³• 2 å°è£…åˆ°ä¸‹å±‚, å¾—åˆ°æœ€ç»ˆå®žçŽ°: æ–­å¼€è®¡ç®—å›¾çš„è¿žæŽ¥
        self.auto_diff_obj.close()

        return g

    @property
    def param_num(self):
        if not self.__param_num:
            if not self.params:
                self.__param_num = 0
            num = 0
            for v in self.params.values():
                num += v.size

            self.__param_num = num
        return self.__param_num


class Dense(Layer):
    def __init__(self,
                 num_in,
                 num_out,
                 w_init=RandomInitializer(),
                 b_init=ZeroInitializer()):
        super().__init__("Dense")

        self.in_shape = num_in
        self.out_shape = num_out

        self.params = {
            "w": w_init([num_in, num_out]),
            "b": b_init([1, num_out]),
        }

    # @profile    # https://github.com/pythonprofilers/memory_profiler
    def forward(self, inputs):
        # self.inputs = inputs
        return inputs @ self.params["w"] + self.params["b"]

    # Do not override backward, use Layer.backward (auto diff)
    # def backward(self, grads):
    #     self.grads["w"] = self.inputs.T @ grads
    #     self.grads["b"] = np.sum(grads, axis=0)
    #     return grads @ self.params["w"].T


class Activation(Layer):
    """Base Activation Layer"""

    def __init__(self, name):
        super().__init__(name)
        self.inputs = None

    def func(self, x):
        raise NotImplementedError

    def derivative_func(self, x):
        raise NotImplementedError

    def forward(self, inputs):
        # self.inputs = inputs
        return tensor(self.func(inputs))

    def backward(self, grads):
        # self.grads['df'] = tensor(self.derivative_func(self.inputs)) * grads
        # return self.grads['df']
        return tensor(self.derivative_func(self.inputs)) * grads


class Sigmoid(Activation):
    def __init__(self):
        super().__init__("Sigmoid")

    def func(self, x):
        return 1 / (1 + np.exp(-x))

    def derivative_func(self, x):
        return self.func(x) * (1 - self.func(x))


class ReLU(Activation):
    def __init__(self):
        super().__init__("ReLU")

    def func(self, x):
        return np.maximum(x, 0)

    def derivative_func(self, x):
        return x > 0


class Dropout(Layer):
    def __init__(self, keep_prob=0.5):
        super().__init__("Dropout")
        self._keep_prob = keep_prob
        self._multiplier = None

    def forward(self, inputs):
        multiplier = np.random.binomial(
            1, self._keep_prob, size=inputs.shape)
        self._multiplier = multiplier / self._keep_prob
        outputs = inputs * self._multiplier
        return tensor(outputs)

    def backward(self, grad):
        # self.grads['grad'] = grad * self._multiplier
        # return self.grads['grad']
        return tensor(grad * self._multiplier)
